#  Mentor Class

## Quick Navigation

- [overview](1-overview.md)
- [backend-api-(app.py)](2-backend-api-(app.py).md)
  - [api-endpoints](2.1-api-endpoints.md)
  - [mentor-class](2.2-mentor-class.md)
- [user-interfaces](3-user-interfaces.md)
  - [chat-interface](3.1-chat-interface.md)
  - [progress-dashboard](3.2-progress-dashboard.md)
  - [admin-interface](3.3-admin-interface.md)
- [database-architecture](4-database-architecture.md)
  - [user-and-progress-models](4.1-user-and-progress-models.md)
  - [database-migrations](4.2-database-migrations.md)
- [rag-system](5-rag-system.md)
  - [vector-database](5.1-vector-database.md)
  - [language-models](5.2-language-models.md)
- [deployment-and-configuration](6-deployment-and-configuration.md)
  - [dependencies](6.1-dependencies.md)
  - [audio-processing-features](6.2-audio-processing-features.md)

## Table of Contents

- [Mentor Class](#mentor-class)
  - [Purpose and Scope](#purpose-and-scope)
  - [Class Structure and Components](#class-structure-and-components)
  - [Initialization Flow](#initialization-flow)
  - [Document Ingestion Process](#document-ingestion-process)
  - [Query Processing Flow](#query-processing-flow)
  - [LLM Integration](#llm-integration)
  - [Vector Database Integration](#vector-database-integration)
  - [Session Management Integration](#session-management-integration)
  - [API Reference](#api-reference)
  - [Usage in the Flask Application](#usage-in-the-flask-application)
  - [Interactions with User Progress Data](#interactions-with-user-progress-data)
  - [Technical Considerations](#technical-considerations)

# Mentor Class

Relevant source files

* [api/app.py](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py)

## Purpose and Scope

The Mentor class is a core component of the MARS-labs system that manages the Retrieval-Augmented Generation (RAG) pipeline. It handles document ingestion, vector embeddings, and question answering using either Ollama or Gemini language models. This class serves as the knowledge engine behind the system's conversational capabilities, retrieving relevant context from ingested documents to provide accurate, contextually appropriate responses to user queries.

For information about the API endpoints that utilize this class, see [API Endpoints](/JATAYU000/MARS-labs/2.1-api-endpoints).

Sources: [api/app.py95-274](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L95-L274)

## Class Structure and Components

The Mentor class combines several components to create a complete RAG pipeline:

Key properties:

* `selected_model`: Determines which LLM to use (Ollama or Gemini)
* `text_splitter`: Component that chunks documents for processing
* `vector_store`: ChromaDB instance that stores document embeddings
* `retriever`: Component that retrieves relevant documents from the vector store
* `model`: The language model instance (either Ollama or Gemini)

Sources: [api/app.py96-148](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L96-L148)

## Initialization Flow

When a Mentor instance is created, it performs the following initialization sequence:

Sources: [api/app.py96-148](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L96-L148) [api/app.py151-183](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L151-L183)

## Document Ingestion Process

The Mentor class processes documents through a multi-step pipeline:

Sources: [api/app.py185-205](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L185-L205)

## Query Processing Flow

When a user submits a query, the Mentor processes it as follows:

Sources: [api/app.py207-254](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L207-L254)

## LLM Integration

The Mentor class supports two language models:

| Model | Implementation | Configuration | Usage |
| --- | --- | --- | --- |
| Ollama | RemoteOllamaWrapper | Uses ngrok tunnel to connect to remote Ollama server | Formats prompts and retrieves responses through HTTP |
| Gemini | Google GenerativeAI | Configured with API key from environment variables | Uses the Google Generative AI Python client |

The class handles:

* Model initialization and configuration
* Prompt formatting specific to each model
* Response processing
* Error handling

Sources: [api/app.py62-92](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L62-L92) [api/app.py134-145](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L134-L145) [api/app.py219-225](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L219-L225)

## Vector Database Integration

The Mentor class uses ChromaDB as its vector database:

* Embeddings are generated using HuggingFace's e5-base model
* The vector store is persisted to disk in the "chroma\_db" directory
* Document retrievals use similarity search with configurable parameters
* The retriever is configured with a similarity score threshold

Sources: [api/app.py151-183](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L151-L183) [api/app.py335-336](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L335-L336)

## Session Management Integration

The Mentor class is used within the Flask application's session management system:

The Flask application maintains a `chat_instances` dictionary that maps session IDs to Mentor instances, ensuring each user session has its own RAG pipeline.

Sources: [api/app.py299](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L299-L299) [api/app.py340-352](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L340-L352) [api/app.py524-529](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L524-L529)

## API Reference

The Mentor class exposes the following methods:

| Method | Purpose | Parameters | Return Value |
| --- | --- | --- | --- |
| `__init__` | Initialize a new Mentor instance | `llm_model` (model name), `selected_model` ("ollama" or "gemini") | None |
| `init_vector_store` | Set up the ChromaDB vector store | None | None |
| `ingest` | Process and store a document | `file_path` (path to PDF) | Number of chunks processed |
| `ask` | Process a query and generate a response | `query` (user question) | Response string |
| `generate_answer_with_gemini` | Use Gemini to answer a query | `query`, `context` | Generated text response |
| `clear` | Clear the vector store | None | None |
| `load_embeddings` | Load pre-generated embeddings | `path` (to NPY file) | Number of embeddings loaded |

Sources: [api/app.py95-274](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L95-L274)

## Usage in the Flask Application

The Mentor class is used in several Flask routes:

* `/upload`: Accepts document uploads, uses `mentor.ingest()` to process them
* `/ask`: Processes user questions, uses `mentor.ask()` to generate responses
* `/clear`: Clears conversation history and vector store using `mentor.clear()`

Sources: [api/app.py456-507](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L456-L507) [api/app.py510-561](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L510-L561) [api/app.py565-574](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L565-L574)

## Interactions with User Progress Data

When processing queries, the Mentor class receives user progress data that gets incorporated into the prompt:

This allows responses to be personalized based on the user's learning progress.

Sources: [api/app.py536-554](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L536-L554)

## Technical Considerations

1. **Model Switching**: The system allows switching between Ollama and Gemini models during a session.
2. **Persistent Storage**: Document embeddings are stored persistently in ChromaDB.
3. **Session Isolation**: Each user session has an isolated Mentor instance to prevent cross-talk.
4. **Memory Management**: Inactive sessions are cleaned up periodically to free resources.
5. **Error Handling**: The class includes comprehensive error handling to prevent crashes.

Sources: [api/app.py524-529](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L524-L529) [api/app.py643-653](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L643-L653)