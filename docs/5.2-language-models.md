#  Language Models

## Quick Navigation

- [overview](1-overview.md)
- [backend-api-(app.py)](2-backend-api-(app.py).md)
  - [api-endpoints](2.1-api-endpoints.md)
  - [mentor-class](2.2-mentor-class.md)
- [user-interfaces](3-user-interfaces.md)
  - [chat-interface](3.1-chat-interface.md)
  - [progress-dashboard](3.2-progress-dashboard.md)
  - [admin-interface](3.3-admin-interface.md)
- [database-architecture](4-database-architecture.md)
  - [user-and-progress-models](4.1-user-and-progress-models.md)
  - [database-migrations](4.2-database-migrations.md)
- [rag-system](5-rag-system.md)
  - [vector-database](5.1-vector-database.md)
  - [language-models](5.2-language-models.md)
- [deployment-and-configuration](6-deployment-and-configuration.md)
  - [dependencies](6.1-dependencies.md)
  - [audio-processing-features](6.2-audio-processing-features.md)

## Table of Contents

- [Language Models](#language-models)
  - [Supported Language Models](#supported-language-models)
  - [Language Model Architecture](#language-model-architecture)
  - [Model Integration Details](#model-integration-details)
    - [RemoteOllamaWrapper](#remoteollamawrapper)
    - [Gemini Integration](#gemini-integration)
  - [Model Selection and Configuration](#model-selection-and-configuration)
    - [Model Selection Process](#model-selection-process)
    - [Configuration Requirements](#configuration-requirements)
  - [Prompt Templates and RAG Integration](#prompt-templates-and-rag-integration)
    - [Prompt Structure](#prompt-structure)
    - [RAG Integration Flow](#rag-integration-flow)
  - [Performance and Usage Considerations](#performance-and-usage-considerations)
    - [Model Comparison](#model-comparison)
    - [Usage in the Application](#usage-in-the-application)
  - [Technical Implementation](#technical-implementation)

# Language Models

Relevant source files

* [api/app.py](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py)

This document provides technical documentation on the language model integrations used in the MARS-labs Retrieval-Augmented Generation (RAG) system. It covers the supported models, their integration methods, configuration requirements, and usage patterns within the application. For information about the overall RAG system architecture, see [RAG System](/JATAYU000/MARS-labs/5-rag-system).

## Supported Language Models

The MARS-labs platform integrates two language model options that can be selected at runtime:

1. **Ollama** - A self-hosted model accessed through a remote API wrapper
2. **Gemini** - Google's Generative AI model accessed through the official API

The system is designed to allow seamless switching between these models based on user preference or specific requirements.

Sources: [api/app.py38-39](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L38-L39) [api/app.py134-145](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L134-L145)

## Language Model Architecture

The diagram above illustrates how language models are integrated within the Mentor class, which serves as the central component for RAG functionality. When a query is processed, the system retrieves relevant context from the vector store, formats a prompt with that context, and then routes the prompt to the selected language model.

Sources: [api/app.py95-132](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L95-L132) [api/app.py207-229](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L207-L229)

## Model Integration Details

### RemoteOllamaWrapper

The `RemoteOllamaWrapper` class provides an abstraction layer for communicating with a remote Ollama server through an ngrok tunnel.

Key implementation details:

* The wrapper sends requests to the `/api/generate` endpoint on the remote Ollama server
* Responses are streamed as individual tokens and concatenated into a single text response
* Custom headers are used to ensure proper ngrok tunnel communication

Sources: [api/app.py62-92](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L62-L92) [api/app.py43-60](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L43-L60) [api/app.py222-223](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L222-L223)

### Gemini Integration

The Gemini model is integrated using Google's official Generative AI Python library.

Key implementation details:

* The API key is loaded from environment variables
* Gemini 2.0 Flash model is used by default
* The API call happens through the `generate_content` method
* Response text is extracted directly from the API response

Sources: [api/app.py104-105](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L104-L105) [api/app.py142-143](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L142-L143) [api/app.py231-255](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L231-L255)

## Model Selection and Configuration

### Model Selection Process

The model selection happens at two levels:

1. **Initial selection** - When a session is created, the Mentor class is initialized with the default model set in the global `selected_model` variable
2. **Runtime switching** - Users can change models through the chat interface, which triggers re-initialization of the Mentor instance

Sources: [api/app.py38](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L38-L38) [api/app.py134-145](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L134-L145) [api/app.py525-527](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L525-L527)

### Configuration Requirements

| Model | Required Configuration | Environment Variables | Default Setting |
| --- | --- | --- | --- |
| Ollama | Remote server URL | `NGROK_URI` | None |
|  | Model name | `OLLAMA_MODEL` | None |
| Gemini | API Key | `GEMINI_API_KEY` | None |

The language model configuration is loaded from environment variables at application startup.

Sources: [api/app.py40-43](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L40-L43) [api/app.py104-105](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L104-L105)

## Prompt Templates and RAG Integration

Both language models use a standardized prompt template structure that includes:

1. Retrieved context from the vector database
2. The user's query
3. Contextual information (user progress, chat history)
4. Specific instructions for generating responses

The prompt template is designed to provide consistent results regardless of which language model is used.

### Prompt Structure

```
You are an expert mentor providing personalized guidance based on the following context.

CONTEXT: {context}

USER QUERY: {question}

INSTRUCTIONS:
1. Prioritize information from the provided context, but supplement with general knowledge when necessary.
2. Deliver concise but sufficiently explanatory, factually accurate responses with examples(if available) that directly address the query.
3. Do not mention the text, if it is from the text or outside the text, just give the response and don't specify whether it is form the text or outside.
4. Consider the chat history (included in the query) to personalize your guidance.
5. Maintain a supportive, encouraging tone throughout.
6. If the user's input is not in English, respond in the same language.
7. If chat history shows a model switch occurred, adjust your response style accordingly.
8. Remember this is a RAG (Retrieval-Augmented Generation) implementation using two models.

Respond in a clear, helpful manner that builds the user's confidence while providing accurate information.

```

Sources: [api/app.py107-126](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L107-L126) [api/app.py233-251](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L233-L251)

### RAG Integration Flow

The RAG integration process follows these key steps:

1. The user query is received by the `ask` method in the Mentor class
2. Relevant documents are retrieved from the vector store
3. Documents are combined into a context string
4. Based on the selected model, the appropriate generation method is called
5. The generated response is returned to the user

Sources: [api/app.py207-229](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L207-L229) [api/app.py231-255](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L231-L255)

## Performance and Usage Considerations

### Model Comparison

| Feature | Ollama | Gemini |
| --- | --- | --- |
| Hosting | Self-hosted (remote) | Cloud API |
| Connectivity | Ngrok tunnel | Direct HTTPS |
| Authentication | Headers | API Key |
| Response format | Token stream | Complete response |
| Cost | Free (self-hosted) | Usage-based pricing |
| Setup complexity | Higher (requires Ollama server) | Lower (just API key) |

### Usage in the Application

The language models are primarily used in two contexts:

1. **Direct questions** - When users ask questions through the chat interface
2. **RAG-enhanced responses** - When questions are combined with retrieved context to generate more accurate answers

The model selection can be made at runtime through the UI, allowing users to compare responses between different models for the same question.

Sources: [api/app.py510-561](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L510-L561)

## Technical Implementation

The Flask application maintains a dictionary of chat instances, each associated with a unique session ID. This allows multiple users to interact with the system concurrently, each with their own language model instance.

When a user makes a request, the appropriate chat instance is retrieved using their session ID:

This design pattern allows for efficient resource usage while maintaining isolation between different user sessions.

Sources: [api/app.py299](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L299-L299) [api/app.py339-352](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L339-L352) [api/app.py524-529](https://github.com/JATAYU000/MARS-labs/blob/d77026a0/api/app.py#L524-L529)